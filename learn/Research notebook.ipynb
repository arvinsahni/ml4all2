{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%file utils.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "error_messages = {\n",
    "    \"No clear target in training data\": \n",
    "        (\"The training data must have \" \n",
    "         \"exactly one more column than \" \n",
    "         \"the test data.\"),\n",
    "    \"Training data has too many columns\":\n",
    "        (\"The training data has more \"\n",
    "         \"than one column different than \"\n",
    "         \"the testing data: %s\"),\n",
    "    \"Column names inconsistent\":\n",
    "        (\"The training columns and the \"\n",
    "         \"test columns must have \"\n",
    "         \"identical names excepts for \"\n",
    "         \"the target variables. \"\n",
    "         \"Different columns: %s\")\n",
    "    }\n",
    "\n",
    "def X_y_split(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Determines which variables are the target\n",
    "    and which are the features. Returns just\n",
    "    The X and y data in the training dataset\n",
    "    as a tuple.\n",
    "    \n",
    "    Example usage:\n",
    "    X, y = learn.X_y_split(X_train, X_test)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: pandas dataframe\n",
    "        The data that has the target in it.\n",
    "    \n",
    "    X_test: pandas dataframe\n",
    "        The data that does not have the target in it.\n",
    "    \"\"\"\n",
    "    X_train = X_train.copy()\n",
    "    n_train_cols = X_train.shape[1]\n",
    "    n_test_cols = X_test.shape[1]\n",
    "    \n",
    "    if n_train_cols != n_test_cols + 1:\n",
    "        msg = error_messages[\"No clear target in training data\"]\n",
    "        raise ValueError(msg)\n",
    "        \n",
    "    test_columns = set(X_test.columns)\n",
    "    train_columns = set(X_train.columns)\n",
    "    target_columns = train_columns - test_columns\n",
    "    if len(target_columns) > 1:\n",
    "        key = \"Training data has too many columns\"\n",
    "        msg_ = error_messages[key]\n",
    "        msg = msg_ % str(target_columns)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    extra_columns_in_test = test_columns - train_columns\n",
    "    if extra_columns_in_test:\n",
    "        key = \"Column names inconsistent\"\n",
    "        msg_ = error_messages[key]\n",
    "        msg = msg_ % str(extra_columns_in_test)\n",
    "        raise ValueError(msg)     \n",
    "\n",
    "    y_name = target_columns.pop()\n",
    "    y = X_train.pop(y_name)\n",
    "    return X_train, y\n",
    "\n",
    "\n",
    "def X_to_train_test(X, target_name, test_size=.05):\n",
    "    X = X.copy()\n",
    "    y = X.pop(target_name)\n",
    "    X_train, X_test, y_train, _ = train_test_split(X, \n",
    "                                                   y, \n",
    "                                                   test_size=test_size,\n",
    "                                                   random_state=42)\n",
    "    X_train[target_name] = y_train\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def make_data(source):\n",
    "    \"\"\"\n",
    "    Utility function to assist in loading different \n",
    "    sample datasets. Returns training data (that \n",
    "    contains the target) and testing data (that\n",
    "    does not contain the target).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    source: string, optional (default=\"boston\")\n",
    "        The specific dataset to load. Options:\n",
    "        - Regression: \"boston\", \"diabetes\"\n",
    "        - Classification: \"cancer\", \"digits\", \"iris\", \"titanic\"\n",
    "    \"\"\"\n",
    "    if source == \"boston\":\n",
    "        data = datasets.load_boston()\n",
    "    elif source == \"diabetes\":\n",
    "        data = datasets.load_diabetes()\n",
    "        data[\"feature_names\"] = [\"f{}\".format(v) \n",
    "                                 for v in range(10)]\n",
    "    elif source == \"cancer\":\n",
    "        data = datasets.load_breast_cancer()\n",
    "    elif source == \"digits\":\n",
    "        data = datasets.load_digits()\n",
    "        data[\"feature_names\"] = [\"f{}\".format(v) \n",
    "                                 for v in range(64)]        \n",
    "    elif source == \"iris\":\n",
    "        data = datasets.load_iris()\n",
    "    elif source == \"titanic\":\n",
    "        train_data_path = \"../tests/test_data/titanic/train.csv\"\n",
    "        test_data_path = \"../tests/test_data/titanic/test.csv\"\n",
    "\n",
    "        X_train = pd.read_csv(train_data_path)\n",
    "        X_test = pd.read_csv(test_data_path)\n",
    "        return X_train, X_test\n",
    "    elif source == \"abalone\":\n",
    "        train_data_path = \"../tests/test_data/abalone_age/abalone.data\"\n",
    "        col_names = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \n",
    "                     \"Whole_weight\", \"Shucked_weight\", \n",
    "                     \"Viscera_weight\", \"Shell_weight\", \"Rings\"]\n",
    "        X = pd.read_csv(train_data_path, header=None, names=col_names)\n",
    "        X[\"Rings\"] = (X.Rings >= 9).astype(int)\n",
    "        return X_to_train_test(X, \"Rings\")\n",
    "    elif source == \"bank_marketing\":\n",
    "        train_data_path = \"../tests/test_data/bank_marketing/bank-full.csv\"\n",
    "        X = pd.read_csv(train_data_path, sep=\";\")\n",
    "        return X_to_train_test(X, \"y\")\n",
    "    elif source == \"car_evaluation\":\n",
    "        train_data_path = \"../tests/test_data/car_evaluation/car.data\"\n",
    "        col_names = [\"buying\", \"maint\", \"doors\", \n",
    "                     \"persons\", \"lug_boot\", \"safety\", \"car_evaluation\"]\n",
    "        X = pd.read_csv(train_data_path, header=None, names=col_names)\n",
    "        return X_to_train_test(X, \"car_evaluation\")\n",
    "    elif source == \"income\":\n",
    "        train_data_path = \"../tests/test_data/census_income/adult.data\"\n",
    "        col_names = [\"age\", \"workclass\", \"fnlwgt\", \n",
    "                     \"education\", \"education-num\", \n",
    "                     \"marital-status\", \"occupation\", \n",
    "                     \"relationship\", \"race\", \"sex\",\n",
    "                     \"capital-gain\", \"capital-loss\", \n",
    "                     \"hours-per-week\", \"native-country\",\n",
    "                     \"income\"]\n",
    "        train = pd.read_csv(train_data_path, skiprows=[0], \n",
    "                            header=None, names=col_names)\n",
    "        test_data_path = \"../tests/test_data/census_income/adult.test\"\n",
    "        test = pd.read_csv(test_data_path, skiprows=[0], \n",
    "                           header=None, names=col_names)\n",
    "        X = pd.concat([train,test])\n",
    "        return X_to_train_test(X, \"income\")\n",
    "    elif source == \"chess\":\n",
    "        train_data_path = \"../tests/test_data/chess/kr-vs-kp.data\"\n",
    "        X = pd.read_csv(train_data_path, header=None)\n",
    "        return X_to_train_test(X, 36)\n",
    "    elif source == \"mushrooms\":\n",
    "        train_data_path = \"../tests/test_data/mushroom/agaricus-lepiota.data\"\n",
    "        X = pd.read_csv(train_data_path, header=None)\n",
    "        return X_to_train_test(X, 0)\n",
    "    elif source == \"tictactoe\":\n",
    "        train_data_path = \"../tests/test_data/tictactoe/tic-tac-toe.data\"\n",
    "        X = pd.read_csv(train_data_path, header=None)\n",
    "        return X_to_train_test(X, 9)\n",
    "    elif source == \"wine-origin\":\n",
    "        train_data_path = \"../tests/test_data/wine_origin/wine.data\"\n",
    "        X = pd.read_csv(train_data_path, header=None)\n",
    "        return X_to_train_test(X, 0)\n",
    "    elif source == \"wine-quality\":\n",
    "        train_data_path = \"../tests/test_data/wine_quality/winequality-white.csv\"\n",
    "        X = pd.read_csv(train_data_path, sep=\";\")\n",
    "        X[\"quality\"] = (X.quality > 5).astype(int)\n",
    "        return X_to_train_test(X, \"quality\")\n",
    "    else:\n",
    "        raise ValueError(\"Not a valid dataset.\")\n",
    "    X = pd.DataFrame(data=data.data, \n",
    "                     columns=data.feature_names)\n",
    "    y = pd.Series(data=data.target)\n",
    "    X_train, X_test, y_train, _ = train_test_split(X, \n",
    "                                                   y, \n",
    "                                                   test_size=.05,\n",
    "                                                   random_state=42)\n",
    "    X_train[\"target\"] = y_train\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def is_categorical(x, \n",
    "                   max_classes=\"auto\", \n",
    "                   strings_are_categorical=True):\n",
    "    \"\"\"\n",
    "    Check if a target variable is a classification\n",
    "    problem or a regression problem. Returns True if\n",
    "    classification and False if regression. On failure,\n",
    "    raises a ValueError.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: array-like\n",
    "        This should be the target variable. Ideally, \n",
    "        you should convert it to be numeric before \n",
    "        using this function.\n",
    "        \n",
    "    max_classes: int or float, optional (default=\"auto\")\n",
    "        Determines the max number of unique values\n",
    "        there can be for it being a categorical variable\n",
    "        \n",
    "        If \"auto\" - sets it equal to 10% of the dataset or\n",
    "            100, whichever is smaller\n",
    "        If float - interprets as percent of dataset size\n",
    "        If int - interprets as number of classes\n",
    "        \n",
    "    strings_are_categorical: bool, optional (default=True)\n",
    "        If a variable is a string and cannot be coerced\n",
    "        to a number, returns True regardless of the number\n",
    "        of unique values. \n",
    "    \"\"\"\n",
    "    x = pd.Series(x)\n",
    "    n = len(x)\n",
    "    n_unique = len(x.unique())\n",
    "    if max_classes == \"auto\":\n",
    "        auto_n_classes = .05\n",
    "        n_max_classes = int(n*auto_n_classes)\n",
    "        max_classes = min(n_max_classes, 100)\n",
    "    if isinstance(max_classes, float):\n",
    "        n_max_classes = int(n*max_classes)\n",
    "        max_classes = min(n_max_classes, int(n/2))\n",
    "    # If x is numeric\n",
    "    if x.dtype.kind in 'bifc':\n",
    "        # If there are more than max_classes\n",
    "        # classify as a regression problem\n",
    "        if n_unique > max_classes:\n",
    "            return False\n",
    "        # If there are floating point numbers\n",
    "        # classify as a regression problem\n",
    "        decimals = (x - x.astype(int)).mean()\n",
    "        if decimals > .01:\n",
    "            return False\n",
    "    if n_unique <= max_classes:\n",
    "        return True\n",
    "    try:\n",
    "        x.astype(float)\n",
    "        return False\n",
    "    except ValueError:\n",
    "        if strings_are_categorical:\n",
    "            return True\n",
    "        msg = (\"Malformed data. \"\n",
    "               \"Variable is non-numeric \"\n",
    "               \"and there are more \"\n",
    "               \"unique values than allowed \"\n",
    "               \"by max_classes\")\n",
    "        raise ValueError(msg)\n",
    "        \n",
    "        \n",
    "def categorical_columns(X):\n",
    "    \"\"\"Returns a list of all categorical columns\"\"\"\n",
    "    cats = X.apply(is_categorical, axis=0)\n",
    "    categoricals = cats[cats].index.tolist()\n",
    "    return categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../tests/test_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%file ../tests/test_utils.py\n",
    "import unittest\n",
    "from learn import utils\n",
    "\n",
    "class TestUtils(unittest.TestCase):\n",
    "    def test_making_data_simple(self):\n",
    "        for data in [\"boston\", \"iris\"]:\n",
    "            X_train, X_test = utils.make_data(source=data)\n",
    "            train_cols = X_train.columns\n",
    "            test_cols = X_test.columns\n",
    "            # Training data should have exactly one additional column\n",
    "            self.assertEqual(len(train_cols), len(test_cols)+1)\n",
    "            # Ensure only one column name is different\n",
    "            n_diff_cols = len(set(X_train.columns) - set(X_test.columns))\n",
    "            self.assertEqual(1, n_diff_cols)\n",
    "        \n",
    "    def test_is_classification_problem(self):\n",
    "        # Shorten function name\n",
    "        icp = utils.is_categorical\n",
    "        # Regression because floats\n",
    "        result = icp([1.1, 2.1])\n",
    "        self.assertEqual(result, 0)\n",
    "        # Regression because number of unique\n",
    "        result = icp([1,2,3,4])\n",
    "        self.assertEqual(result, 0)\n",
    "        # Classification because words\n",
    "        result = icp([\"cat\"]*20+[\"dog\"]*20)\n",
    "        self.assertEqual(result, 1)\n",
    "        # Classification because number of uniques\n",
    "        result = icp([0]*20+[1]*20)\n",
    "        self.assertEqual(result, 1)\n",
    "        # Real data tests - Regression\n",
    "        for dataset in [\"boston\", \"diabetes\"]:\n",
    "            data = utils.make_data(source=dataset)\n",
    "            X, y = utils.X_y_split(*data)\n",
    "            self.assertEqual(icp(y), 0)\n",
    "        # Real data tests - Classification\n",
    "        for dataset in [\"cancer\", \"digits\", \"iris\"]:\n",
    "            data = utils.make_data(source=dataset)\n",
    "            X, y = utils.X_y_split(*data)\n",
    "            self.assertEqual(icp(y), 1)\n",
    "            \n",
    "# class TestXYSplit(unittest.TestCase):\n",
    "#     pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting forall.py\n"
     ]
    }
   ],
   "source": [
    "%%file forall.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, RegressorMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "from sklearn import metrics\n",
    "from learn import utils\n",
    "\n",
    "def categorical_unique_counts(X):\n",
    "    \"\"\"\n",
    "    Returns series of categorical columns\n",
    "    and count of unique values in each \n",
    "    column\n",
    "    \"\"\"\n",
    "    cats = utils.categorical_columns(X)\n",
    "    return X[cats].apply(pd.Series.nunique, axis=0)\n",
    "\n",
    "def small_categorical(X, large_class_threshold=10):\n",
    "    counts = categorical_unique_counts(X)\n",
    "    mask = counts < large_class_threshold\n",
    "    return counts[mask].index.tolist()\n",
    "\n",
    "def large_categorical(X, large_class_threshold=10):\n",
    "    counts = categorical_unique_counts(X)\n",
    "    mask = counts>=large_class_threshold\n",
    "    return counts[mask].index.tolist()\n",
    "\n",
    "def word_to_num(word, max_char=5):\n",
    "    \"\"\"\n",
    "    Assigns a number to a word that\n",
    "    is the approximate sort order of\n",
    "    the word\n",
    "    \n",
    "    Words with the same first max_char\n",
    "    will have the same value.\n",
    "    \"\"\"\n",
    "    word_val = 0\n",
    "    for n, char in enumerate(str(word)):\n",
    "        if n > max_char:\n",
    "            break\n",
    "        num = ord(char)/130\n",
    "        den = 10**n\n",
    "        total = num/den\n",
    "        word_val += total\n",
    "    return word_val\n",
    "\n",
    "def word_size(word):\n",
    "    \"\"\"\n",
    "    Returns the length of the word\n",
    "    \"\"\"\n",
    "    return len(str(word))\n",
    "\n",
    "class CategoricalImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Adds a new \"Missing\" category for missing values\n",
    "    \"\"\"\n",
    "    def __init__(self, fill_value=\"NULL\"):\n",
    "        self.fill_value = fill_value\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.cat_cols = utils.categorical_columns(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        fill_values = {c:self.fill_value for c in self.cat_cols}\n",
    "        return X.fillna(fill_values, axis=0)\n",
    "\n",
    "class NumericImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    TODO: Add option for indicator variable if NaN\n",
    "    \"\"\"\n",
    "    def __init__(self, method=\"mean\"):\n",
    "        self.method = method\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.method == \"mean\":\n",
    "            self.fill_values = X.mean()\n",
    "        if self.method == \"max\":\n",
    "            self.fill_values = X.max() + 1\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X[~pd.np.isfinite(X)] = 0 #TODO: Fix\n",
    "        return X.fillna(self.fill_values)\n",
    "\n",
    "    \n",
    "class KeepNumeric(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.numeric_columns = X.dtypes[X.dtypes != \"object\"].index.tolist()\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.numeric_columns]\n",
    "    \n",
    "    \n",
    "class Categoricals(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, large_class_threshold=10):\n",
    "        \"\"\"\n",
    "        Anything large_class_threshold and larger\n",
    "        will be treated as a categorical features\n",
    "        with a large number of categories.\n",
    "        \"\"\"\n",
    "        self.large_class_threshold = large_class_threshold\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        lct = self.large_class_threshold\n",
    "        self.small = small_categorical(X, lct)\n",
    "        self.large = large_categorical(X, lct)\n",
    "        self.all = self.small + self.large\n",
    "        # Save category unique value counts for feature\n",
    "        # engineering\n",
    "        self.value_counts = {}\n",
    "        for col in self.large:\n",
    "            self.value_counts[col] = X[col].value_counts()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Add sort value based features\n",
    "        for col in self.all:\n",
    "            new_col = X[col].apply(word_to_num)\n",
    "            col_name = str(col)+\"__sort\"\n",
    "            X[col_name] = new_col\n",
    "        \n",
    "        for col in self.large:\n",
    "            # Add count based features\n",
    "            counts = self.value_counts[col]\n",
    "            X = X.join(counts, \n",
    "                       on=col, \n",
    "                       rsuffix=\"__counts\")\n",
    "            # Add word length features\n",
    "            new_col = X[col].apply(word_size)\n",
    "            col_name = str(col)+\"__length\"\n",
    "            X[col_name] = new_col\n",
    "        return X\n",
    "\n",
    "    \n",
    "class Standardize(BaseException, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.mean = X.mean(axis=0)\n",
    "        self.std = X.std(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return (X - self.mean)/self.std\n",
    "    \n",
    "\n",
    "class DropBadColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Drops columns with:\n",
    "        * NaN standard deviation\n",
    "        * Zero standard deviation\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        std = X.std(axis=0)\n",
    "        null_std = std.isnull()\n",
    "        zero_std = std == 0\n",
    "        bad_std_cols = std[null_std | zero_std].index.values.tolist()\n",
    "        self.to_drop = bad_std_cols\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.drop(self.to_drop, axis=1)\n",
    "    \n",
    "    \n",
    "def regression_metrics(y, y_hat):\n",
    "    exp_var = metrics.explained_variance_score(y, y_hat)\n",
    "    mae = metrics.mean_absolute_error(y, y_hat)\n",
    "    mse = metrics.mean_squared_error(y, y_hat)\n",
    "    medae = metrics.median_absolute_error(y, y_hat)\n",
    "    r2 = metrics.r2_score(y, y_hat)\n",
    "    results = {\n",
    "        \"Explained variance score\": exp_var,\n",
    "        \"Mean absolute error\": mae,\n",
    "        \"Mean squared error\": mse,\n",
    "        \"Root mean squared error\": mse**.5,\n",
    "        \"Median absolute error\": medae,\n",
    "        \"R^2 score\": r2\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "class RegressionPredict(BaseEstimator):\n",
    "    def __init__(self, time_to_compute=100):\n",
    "        self.time_to_compute = time_to_compute\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(X, y)\n",
    "        lr_pred = cross_val_predict(self.lr, X, y, cv=10, n_jobs=-1).reshape(-1, 1)\n",
    "        \n",
    "        self.rf = RandomForestRegressor(n_estimators=self.time_to_compute, \n",
    "                                        random_state=42, \n",
    "                                        oob_score=True, \n",
    "                                        n_jobs=-1)\n",
    "        self.rf.fit(X, y)\n",
    "        rf_pred = self.rf.oob_prediction_.reshape(-1, 1)\n",
    "\n",
    "        layer_1 = np.hstack([\n",
    "            lr_pred, \n",
    "            rf_pred\n",
    "        ])\n",
    "\n",
    "        self.lr_1 = LinearRegression()\n",
    "        self.generalized_predictions = cross_val_predict(self.lr_1, \n",
    "                                                         layer_1, \n",
    "                                                         y, \n",
    "                                                         cv=10, \n",
    "                                                         n_jobs=-1, \n",
    "                                                         method=\"predict\")\n",
    "        self.lr_1.fit(layer_1, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        lr_pred = self.lr.predict(X).reshape(-1, 1)\n",
    "        rf_pred = self.rf.predict(X).reshape(-1, 1)\n",
    "        layer_1 = np.hstack([\n",
    "            lr_pred, \n",
    "            rf_pred\n",
    "        ])\n",
    "        final_predictions = self.lr_1.predict(layer_1)\n",
    "        return final_predictions\n",
    "    \n",
    "\n",
    "class Regression(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, time_to_compute=100):\n",
    "        self.time_to_compute = time_to_compute\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.model = RegressionPredict(time_to_compute=self.time_to_compute)\n",
    "        self.model.fit(X, y)\n",
    "        self.oob_predictions = self.model.generalized_predictions\n",
    "        \n",
    "        self.all_metrics = regression_metrics(y, self.oob_predictions)\n",
    "        self.score_type = \"R^2*100\"\n",
    "        self.score = int(self.all_metrics[\"R^2 score\"]*100)\n",
    "        self.display_score = \"%d/100\" % self.score\n",
    "        self.understandable_metric_name = \"Average prediction error\"\n",
    "        self.understandable_metric_description = \"On average, the predictions will be off by this amount\"\n",
    "        self.understandable_metric_value = self.all_metrics[\"Mean absolute error\"]\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = self.model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "\n",
    "def classification_metrics(y, y_hat):\n",
    "    results = {}\n",
    "    y_prob = y_hat[:, 1]\n",
    "    y_pred = (y_prob > .5).astype(int)\n",
    "    y_bin = label_binarize(y, \n",
    "                           sorted(pd.Series(y).unique()))\n",
    "    binary = y_bin.shape[1] == 1\n",
    "    if binary:\n",
    "        # Fix the binary case returning a column vector\n",
    "        y_bin = np.hstack((-(y_bin - 1), y_bin))\n",
    "    ave_precision = metrics.average_precision_score(y_bin, y_hat)\n",
    "    auc = metrics.roc_auc_score(y_bin, y_hat)\n",
    "    log_loss = metrics.log_loss(y_bin, y_hat)\n",
    "    data = {\n",
    "        \"Accuracy\": (y_hat.argmax(axis=1) == y).mean(),\n",
    "        \"Average precision score\": ave_precision,\n",
    "        \"AUC\": auc,\n",
    "        \"Log loss (cross-entropy loss)\": log_loss\n",
    "            }\n",
    "    if binary:\n",
    "        brier = metrics.brier_score_loss(y, y_prob)\n",
    "        f1 = metrics.f1_score(y, y_pred)\n",
    "        cks = metrics.cohen_kappa_score(y, y_pred)\n",
    "        hamming = metrics.hamming_loss(y, y_pred)\n",
    "        hinge = metrics.hinge_loss(y, y_pred)\n",
    "        jaccard = metrics.jaccard_similarity_score(y, y_pred)\n",
    "        matt = metrics.matthews_corrcoef(y, y_pred)\n",
    "        precision = metrics.precision_score(y, y_pred)\n",
    "        recall = metrics.recall_score(y, y_pred)\n",
    "        binary_data = {\n",
    "            \"Brier score loss\": brier,\n",
    "            \"F1 score\": f1,\n",
    "            \"Cohen's kappa\": cks,\n",
    "            \"Average Hamming loss\": hamming,\n",
    "            \"Hinge loss\": hinge,\n",
    "            \"Jaccard similarity coefficient\": jaccard,\n",
    "            \"Matthews correlation coefficient\": matt,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall\n",
    "            }\n",
    "        data.update(binary_data)\n",
    "    return data\n",
    "\n",
    "\n",
    "class ClassificationPredict(BaseEstimator):\n",
    "    def __init__(self, time_to_compute=100):\n",
    "        self.time_to_compute = time_to_compute\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.lr = LogisticRegression(C=1)\n",
    "        self.lr.fit(X, y)\n",
    "        lr_pred = cross_val_predict(self.lr, X, y, cv=10, n_jobs=-1, method=\"predict_proba\")\n",
    "        \n",
    "        self.rf = RandomForestClassifier(n_estimators=self.time_to_compute, random_state=42, oob_score=True, n_jobs=-1)\n",
    "        self.rf.fit(X, y)\n",
    "        rf_pred = self.rf.oob_decision_function_\n",
    "\n",
    "        layer_1 = np.hstack([\n",
    "            lr_pred, \n",
    "            rf_pred\n",
    "        ])\n",
    "\n",
    "        self.lr_1 = LogisticRegression(C=1)\n",
    "        self.generalized_predictions = cross_val_predict(self.lr_1, \n",
    "                                                         layer_1, \n",
    "                                                         y, \n",
    "                                                         cv=10, \n",
    "                                                         n_jobs=-1, \n",
    "                                                         method=\"predict_proba\")\n",
    "        self.lr_1.fit(layer_1, y)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        lr_pred = self.lr.predict_proba(X)\n",
    "        rf_pred = self.rf.predict_proba(X)\n",
    "        layer_1 = np.hstack([\n",
    "            lr_pred, \n",
    "            rf_pred\n",
    "        ])\n",
    "        final_predictions = self.lr_1.predict_proba(layer_1)\n",
    "        return final_predictions\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = self.predict_proba(X)\n",
    "        return predictions.argmax(1)\n",
    "    \n",
    "\n",
    "class Classification(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, time_to_compute=100):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.time_to_compute = time_to_compute\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        y = pd.Series(y)\n",
    "        self.n_classes = len(y.unique())\n",
    "        self.label_encoder = None\n",
    "        if y.dtype == \"object\":\n",
    "            self.label_encoder = LabelEncoder().fit(y)\n",
    "            y = self.label_encoder.transform(y)\n",
    "\n",
    "        self.model = ClassificationPredict(time_to_compute=self.time_to_compute)\n",
    "        self.model.fit(X, y)\n",
    "        self.oob_predictions = self.model.generalized_predictions\n",
    "        \n",
    "        self.all_metrics = classification_metrics(y, \n",
    "                                                  self.oob_predictions)\n",
    "        self.score_type = \"(AUC - .5)*200\"\n",
    "        self.score = int((self.all_metrics[\"AUC\"] - .5)*200)\n",
    "        self.display_score = \"%d/100\" % self.score\n",
    "        self.understandable_metric_name = \"Accuracy\"\n",
    "        self.understandable_metric_description = \"Each prediction is expected to be correct this often\"\n",
    "        self.understandable_metric_value = self.all_metrics[\"Accuracy\"]\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = self.model.predict(X)\n",
    "        if self.label_encoder is not None:\n",
    "            predictions = self.label_encoder.inverse_transform(predictions)\n",
    "        return predictions\n",
    "\n",
    "    \n",
    "class All():\n",
    "    def __init__(self, time_to_compute=None):\n",
    "        self.time_to_compute = time_to_compute or 100\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X.columns = [str(col) for col in X.columns]\n",
    "        # Determine type of problem\n",
    "        self.classification = utils.is_categorical(y, max_classes=.1)\n",
    "        if self.classification:\n",
    "            model = Classification(time_to_compute=self.time_to_compute)\n",
    "        else:\n",
    "            model = Regression(time_to_compute=self.time_to_compute)\n",
    "        # Create pipeline\n",
    "        steps = [(\"categorical_imputation\", CategoricalImputer()),\n",
    "                 (\"make_categoricals_numeric\", Categoricals()),\n",
    "                 (\"keep_only_numeric\", KeepNumeric()),\n",
    "                 (\"numeric_imputation\", NumericImputer()),\n",
    "                 (\"drop_bad_columns\", DropBadColumns()),\n",
    "                 (\"scale\", Standardize()),\n",
    "                 (\"model\", model)]\n",
    "        pipe = Pipeline(steps)\n",
    "        pipe.fit(X, y)\n",
    "        self.model = pipe\n",
    "        self.score = pipe.named_steps[\"model\"].score\n",
    "        self.score_type = pipe.named_steps[\"model\"].score_type\n",
    "        self.display_score = pipe.named_steps[\"model\"].display_score\n",
    "        self.all_metrics = pipe.named_steps[\"model\"].all_metrics\n",
    "        self.understandable_metric_name = pipe.named_steps[\"model\"].understandable_metric_name\n",
    "        self.understandable_metric_description = pipe.named_steps[\"model\"].understandable_metric_description\n",
    "        self.understandable_metric_value = pipe.named_steps[\"model\"].understandable_metric_value\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X.columns = [str(col) for col in X.columns]\n",
    "        predictions = self.model.predict(X)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "# Allows importing of local modules\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from learn import forall as fa\n",
    "from learn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston: 87/100 (R^2*100)\n",
      "diabetes: 47/100 (R^2*100)\n",
      "cancer: 98/100 ((AUC - .5)*200)\n",
      "digits: 99/100 ((AUC - .5)*200)\n",
      "iris: 97/100 ((AUC - .5)*200)\n",
      "titanic: 73/100 ((AUC - .5)*200)\n",
      "abalone: 81/100 ((AUC - .5)*200)\n",
      "bank_marketing: 85/100 ((AUC - .5)*200)\n",
      "car_evaluation: 99/100 ((AUC - .5)*200)\n",
      "income: 51/100 ((AUC - .5)*200)\n",
      "chess: 99/100 ((AUC - .5)*200)\n",
      "mushrooms: 100/100 ((AUC - .5)*200)\n",
      "tictactoe: 99/100 ((AUC - .5)*200)\n",
      "wine-origin: 100/100 ((AUC - .5)*200)\n",
      "wine-quality: 81/100 ((AUC - .5)*200)\n",
      "1296\n",
      "CPU times: user 46.5 s, sys: 2.62 s, total: 49.1 s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "total_score = 0\n",
    "\n",
    "for dataset in [\"boston\", \"diabetes\", \n",
    "                \"cancer\", \"digits\", \n",
    "                \"iris\", \"titanic\", \n",
    "                \"abalone\", \"bank_marketing\",\n",
    "                \"car_evaluation\", \"income\",\n",
    "                \"chess\", \"mushrooms\",\n",
    "                \"tictactoe\",  \"wine-origin\",\n",
    "                \"wine-quality\"\n",
    "               ]:\n",
    "    # In the flask app:\n",
    "    X_train, X_test = utils.make_data(source=dataset)\n",
    "    X, y = utils.X_y_split(X_train=X_train, X_test=X_test)\n",
    "    model = fa.All()\n",
    "    model.fit(X, y)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"%s: %s (%s)\" % (dataset, model.display_score, model.score_type))\n",
    "    \n",
    "    # Benchmarking\n",
    "    total_score += model.score\n",
    "    \n",
    "print(total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston: 87/100 (R^2*100)\n",
      "diabetes: 47/100 (R^2*100)\n",
      "cancer: 98/100 ((AUC - .5)*200)\n",
      "digits: 99/100 ((AUC - .5)*200)\n",
      "iris: 97/100 ((AUC - .5)*200)\n",
      "titanic: 75/100 ((AUC - .5)*200)\n",
      "abalone: 81/100 ((AUC - .5)*200)\n",
      "bank_marketing: 86/100 ((AUC - .5)*200)\n",
      "car_evaluation: 99/100 ((AUC - .5)*200)\n",
      "income: 51/100 ((AUC - .5)*200)\n",
      "chess: 99/100 ((AUC - .5)*200)\n",
      "mushrooms: 100/100 ((AUC - .5)*200)\n",
      "tictactoe: 99/100 ((AUC - .5)*200)\n",
      "wine-origin: 100/100 ((AUC - .5)*200)\n",
      "wine-quality: 82/100 ((AUC - .5)*200)\n",
      "1300\n",
      "CPU times: user 1min 14s, sys: 3.61 s, total: 1min 17s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "total_score = 0\n",
    "\n",
    "for dataset in [\"boston\", \"diabetes\", \n",
    "                \"cancer\", \"digits\", \n",
    "                \"iris\", \"titanic\", \n",
    "                \"abalone\", \"bank_marketing\",\n",
    "                \"car_evaluation\", \"income\",\n",
    "                \"chess\", \"mushrooms\",\n",
    "                \"tictactoe\",  \"wine-origin\",\n",
    "                \"wine-quality\"\n",
    "               ]:\n",
    "    # In the flask app:\n",
    "    X_train, X_test = utils.make_data(source=dataset)\n",
    "    X, y = utils.X_y_split(X_train=X_train, X_test=X_test)\n",
    "    model = fa.All(time_to_compute=200)\n",
    "    model.fit(X, y)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"%s: %s (%s)\" % (dataset, model.display_score, model.score_type))\n",
    "    \n",
    "    # Benchmarking\n",
    "    total_score += model.score\n",
    "    \n",
    "print(total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston\n",
      "Score: 87/100\n",
      "Average prediction error = 2.18994700093\n",
      "{'Explained variance score': 0.87649582271482873,\n",
      " 'Mean absolute error': 2.1899470009311672,\n",
      " 'Mean squared error': 10.481110565137438,\n",
      " 'Median absolute error': 1.5011250912032263,\n",
      " 'R^2 score': 0.87649574580146905,\n",
      " 'Root mean squared error': 3.2374543340620945}\n",
      "\n",
      "diabetes\n",
      "Score: 47/100\n",
      "Average prediction error = 44.6849187927\n",
      "{'Explained variance score': 0.4738710017729707,\n",
      " 'Mean absolute error': 44.684918792690965,\n",
      " 'Mean squared error': 3062.3986831492375,\n",
      " 'Median absolute error': 39.832219870347757,\n",
      " 'R^2 score': 0.47387075588059335,\n",
      " 'Root mean squared error': 55.338943639621796}\n",
      "\n",
      "cancer\n",
      "Score: 98/100\n",
      "Accuracy = 0.977777777778\n",
      "{'AUC': 0.99214840253012226,\n",
      " 'Accuracy': 0.97777777777777775,\n",
      " 'Average Hamming loss': 0.022222222222222223,\n",
      " 'Average precision score': 0.99223228995832868,\n",
      " 'Brier score loss': 0.017105412294150812,\n",
      " \"Cohen's kappa\": 0.95235364185820792,\n",
      " 'F1 score': 0.98235294117647076,\n",
      " 'Hinge loss': 0.39444444444444443,\n",
      " 'Jaccard similarity coefficient': 0.97777777777777775,\n",
      " 'Log loss (cross-entropy loss)': 0.074654226070965071,\n",
      " 'Matthews correlation coefficient': 0.95238367119307243,\n",
      " 'Precision': 0.97947214076246336,\n",
      " 'Recall': 0.98525073746312686}\n",
      "\n",
      "digits\n",
      "Score: 99/100\n",
      "Accuracy = 0.97539543058\n",
      "{'AUC': 0.99929346883596148,\n",
      " 'Accuracy': 0.97539543057996481,\n",
      " 'Average precision score': 0.99558095543440717,\n",
      " 'Log loss (cross-entropy loss)': 0.14900472051817448}\n",
      "\n",
      "iris\n",
      "Score: 97/100\n",
      "Accuracy = 0.950704225352\n",
      "{'AUC': 0.98699763593380618,\n",
      " 'Accuracy': 0.95070422535211263,\n",
      " 'Average precision score': 0.96849676634794957,\n",
      " 'Log loss (cross-entropy loss)': 0.2073102417026747}\n",
      "\n",
      "titanic\n",
      "Score: 73/100\n",
      "Accuracy = 0.821548821549\n",
      "{'AUC': 0.86956081764824922,\n",
      " 'Accuracy': 0.82154882154882158,\n",
      " 'Average Hamming loss': 0.17845117845117844,\n",
      " 'Average precision score': 0.85462348811101885,\n",
      " 'Brier score loss': 0.12807149999694076,\n",
      " \"Cohen's kappa\": 0.61660618164990133,\n",
      " 'F1 score': 0.75725190839694656,\n",
      " 'Hinge loss': 0.79461279461279466,\n",
      " 'Jaccard similarity coefficient': 0.82154882154882158,\n",
      " 'Log loss (cross-entropy loss)': 0.41435975649212342,\n",
      " 'Matthews correlation coefficient': 0.61811927642654274,\n",
      " 'Precision': 0.792332268370607,\n",
      " 'Recall': 0.72514619883040932}\n",
      "\n",
      "abalone\n",
      "Score: 81/100\n",
      "Accuracy = 0.837449596774\n",
      "{'AUC': 0.90781578923490125,\n",
      " 'Accuracy': 0.83744959677419351,\n",
      " 'Average Hamming loss': 0.16255040322580644,\n",
      " 'Average precision score': 0.90110088432198943,\n",
      " 'Brier score loss': 0.11244689019498243,\n",
      " \"Cohen's kappa\": 0.62733440190852496,\n",
      " 'F1 score': 0.88040051919154461,\n",
      " 'Hinge loss': 0.50126008064516125,\n",
      " 'Jaccard similarity coefficient': 0.83744959677419351,\n",
      " 'Log loss (cross-entropy loss)': 0.36067912418494535,\n",
      " 'Matthews correlation coefficient': 0.62954758736225735,\n",
      " 'Precision': 0.85734922354640664,\n",
      " 'Recall': 0.90472560975609762}\n",
      "\n",
      "bank_marketing\n",
      "Score: 85/100\n",
      "Accuracy = 0.906728754366\n",
      "{'AUC': 0.92794758090571916,\n",
      " 'Accuracy': 0.90672875436554135,\n",
      " 'Average Hamming loss': 0.093271245634458669,\n",
      " 'Average precision score': 0.79734132154962811,\n",
      " 'Brier score loss': 0.066032493469718803,\n",
      " \"Cohen's kappa\": 0.48107085798603921,\n",
      " 'F1 score': 0.53113295880149802,\n",
      " 'Hinge loss': 0.97655413271245639,\n",
      " 'Jaccard similarity coefficient': 0.90672875436554135,\n",
      " 'Log loss (cross-entropy loss)': 0.21869219564053385,\n",
      " 'Matthews correlation coefficient': 0.49018848104603963,\n",
      " 'Precision': 0.64259416595865193,\n",
      " 'Recall': 0.452623179732695}\n",
      "\n",
      "car_evaluation\n",
      "Score: 99/100\n",
      "Accuracy = 0.984156002438\n",
      "{'AUC': 0.99942717906979195,\n",
      " 'Accuracy': 0.98415600243753809,\n",
      " 'Average precision score': 0.99462201880224099,\n",
      " 'Log loss (cross-entropy loss)': 0.070911687132732312}\n",
      "\n",
      "income\n",
      "Score: 51/100\n",
      "Accuracy = 0.570606491659\n",
      "{'AUC': 0.75563602049685774,\n",
      " 'Accuracy': 0.57060649165912325,\n",
      " 'Average precision score': 0.43520980004096177,\n",
      " 'Log loss (cross-entropy loss)': 0.97085735705024756}\n",
      "\n",
      "chess\n",
      "Score: 99/100\n",
      "Accuracy = 0.991765480896\n",
      "{'AUC': 0.9993602745731105,\n",
      " 'Accuracy': 0.99176548089591565,\n",
      " 'Average Hamming loss': 0.0082345191040843219,\n",
      " 'Average precision score': 0.99937520856261053,\n",
      " 'Brier score loss': 0.0072445270021296382,\n",
      " \"Cohen's kappa\": 0.98349737392993841,\n",
      " 'F1 score': 0.99212102111566347,\n",
      " 'Hinge loss': 0.4851778656126482,\n",
      " 'Jaccard similarity coefficient': 0.99176548089591565,\n",
      " 'Log loss (cross-entropy loss)': 0.030211422569306555,\n",
      " 'Matthews correlation coefficient': 0.98349930240086914,\n",
      " 'Precision': 0.99305993690851735,\n",
      " 'Recall': 0.99118387909319894}\n",
      "\n",
      "mushrooms\n",
      "Score: 100/100\n",
      "Accuracy = 1.0\n",
      "{'AUC': 1.0,\n",
      " 'Accuracy': 1.0,\n",
      " 'Average Hamming loss': 0.0,\n",
      " 'Average precision score': 1.0,\n",
      " 'Brier score loss': 9.2490973731272775e-06,\n",
      " \"Cohen's kappa\": 1.0,\n",
      " 'F1 score': 1.0,\n",
      " 'Hinge loss': 0.51794738888168979,\n",
      " 'Jaccard similarity coefficient': 1.0,\n",
      " 'Log loss (cross-entropy loss)': 0.0015835498456632281,\n",
      " 'Matthews correlation coefficient': 1.0,\n",
      " 'Precision': 1.0,\n",
      " 'Recall': 1.0}\n",
      "\n",
      "tictactoe\n",
      "Score: 99/100\n",
      "Accuracy = 0.974725274725\n",
      "{'AUC': 0.99743364012271574,\n",
      " 'Accuracy': 0.9747252747252747,\n",
      " 'Average Hamming loss': 0.025274725274725275,\n",
      " 'Average precision score': 0.99718288218859219,\n",
      " 'Brier score loss': 0.021873258450938113,\n",
      " \"Cohen's kappa\": 0.94361682066754671,\n",
      " 'F1 score': 0.98088113050706571,\n",
      " 'Hinge loss': 0.37142857142857144,\n",
      " 'Jaccard similarity coefficient': 0.9747252747252747,\n",
      " 'Log loss (cross-entropy loss)': 0.09333988002317152,\n",
      " 'Matthews correlation coefficient': 0.94409636282751552,\n",
      " 'Precision': 0.97039473684210531,\n",
      " 'Recall': 0.99159663865546221}\n",
      "\n",
      "wine-origin\n",
      "Score: 100/100\n",
      "Accuracy = 0.00591715976331\n",
      "{'AUC': 1.0,\n",
      " 'Accuracy': 0.0059171597633136093,\n",
      " 'Average precision score': 0.99999999999999989,\n",
      " 'Log loss (cross-entropy loss)': 0.099705793639294854}\n",
      "\n",
      "wine-quality\n",
      "Score: 81/100\n",
      "Accuracy = 0.84332688588\n",
      "{'AUC': 0.90843693799358827,\n",
      " 'Accuracy': 0.84332688588007731,\n",
      " 'Average Hamming loss': 0.15667311411992263,\n",
      " 'Average precision score': 0.89822639786649106,\n",
      " 'Brier score loss': 0.11175614770543406,\n",
      " \"Cohen's kappa\": 0.64028106129079365,\n",
      " 'F1 score': 0.88485231401042486,\n",
      " 'Hinge loss': 0.49323017408123793,\n",
      " 'Jaccard similarity coefficient': 0.84332688588007731,\n",
      " 'Log loss (cross-entropy loss)': 0.35950418527519468,\n",
      " 'Matthews correlation coefficient': 0.64221112558726023,\n",
      " 'Precision': 0.86344019728729959,\n",
      " 'Recall': 0.90735341755749921}\n",
      "\n",
      "CPU times: user 45.9 s, sys: 2.7 s, total: 48.6 s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for dataset in [\"boston\", \"diabetes\", \n",
    "                \"cancer\", \"digits\", \n",
    "                \"iris\", \"titanic\", \n",
    "                \"abalone\", \"bank_marketing\",\n",
    "                \"car_evaluation\", \"income\",\n",
    "                \"chess\", \"mushrooms\",\n",
    "                \"tictactoe\",  \"wine-origin\",\n",
    "                \"wine-quality\"\n",
    "               ]:\n",
    "    # In the flask app:\n",
    "    X_train, X_test = utils.make_data(source=dataset)\n",
    "    X, y = utils.X_y_split(X_train=X_train, X_test=X_test)\n",
    "    model = fa.All(time_to_compute=100)\n",
    "    model.fit(X, y)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(dataset)\n",
    "    print(\"Score: %s\" % model.display_score)\n",
    "    print(model.understandable_metric_name, \"=\", model.understandable_metric_value)\n",
    "    pprint(model.all_metrics)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
